{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model Assumptions\n",
    "\n",
    "Some of the linear regression model assumptions are:\n",
    "\n",
    "- **Linearity**: The mean values of the outcome variable for each increment of the predictor(s) lie along a straight line. In other words, there is a linear relationship between predictors and target.\n",
    "\n",
    "- **No perfect multicollinearity**: There should be no perfect linear relationship between two or more of the predictors. \n",
    "\n",
    "- **Normally distributed errors**: the residuals are random, normally distributed with a mean of 0.\n",
    "\n",
    "- **Homoscedasticity**: At each level of the predictor variable(s), the variance of the residual terms should be constant.\n",
    "\n",
    "Examples of linear models are:\n",
    "\n",
    "- Linear and Logistic Regression\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "\n",
    "**Failure to meet one or more of the model assumptions could end up in a poor model performance**. In other words, the variables do not accurately predict the outcome, with a linear model.\n",
    "\n",
    "If the assumptions are not met, we can:\n",
    "\n",
    "- use a different no-linear model to predict the outcome from the variables\n",
    "- transform the input variables so that they fulfill the assumptions.\n",
    "\n",
    "\n",
    "### Determining if linear model is accurate\n",
    "\n",
    "The main diagnostic to determine if a linear model works well to predict the outcome from the predictors, is to evaluate in the first place, if the error terms, or residuals follow a normal distribution with a mean of zero, and are homoscedastic. If this is true, we can be fairly confident that the model is doing a good job.\n",
    "\n",
    "We can determine normal distribution and homoscedasticity as follows:\n",
    "\n",
    "- Normal distribution can be assessed by Q-Q plots\n",
    "- Homoscedasticity can be assessed by residuals plots\n",
    "\n",
    "\n",
    "If we would also like to test the other assumptions:\n",
    "\n",
    "- Linear regression can be assessed by scatter-plots and residuals plots\n",
    "- Multi-colinearity can be assessed by correlation matrices\n",
    "\n",
    "\n",
    "### If the assumptions are not met\n",
    "\n",
    "Sometimes variable transformation can help the variables meet the model assumptions. We normally do 1 of 2 things:\n",
    "\n",
    "- Mathematical transformation of the variables\n",
    "- Discretisation\n",
    "\n",
    "\n",
    "**I will cover mathematical transformations and discretisation in later sections of the course**. \n",
    "\n",
    "\n",
    "## In this demo...\n",
    "\n",
    "We will:\n",
    "\n",
    "- Train a linear model to predict a target from 3 predictor variables\n",
    "- Evaluate if the model is accurate by examining the residuals\n",
    "- Determine if the residuals are normally distributed\n",
    "- If there is homoscedasticity\n",
    "- We will then transform the data and see how this improves model performance\n",
    "- We will then move on to examine Correlation and Linear relationship between variables and outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DATASC~1\\AppData\\Local\\Temp/ipykernel_17836/2786956607.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# for plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#import matplotlib.pyplot as plt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# for the Q-Q plots\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import seaborn objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrcmod\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\rcmod.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[1;31m# triggering resolution of _auto_backend_sentinel.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m rcParamsDefault = _rc_params_in_file(\n\u001b[1;32m--> 823\u001b[1;33m     \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matplotlibrc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;31m# Strip leading comment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_get_data_path\u001b[1;34m(*args)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for the Q-Q plots\n",
    "import scipy.stats as stats\n",
    "\n",
    "# the dataset for the demo\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# for linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# to split and standarize the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to evaluate the regression model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the the Boston House price data\n",
    "\n",
    "# this is how we load the boston dataset from sklearn\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# create a dataframe with the independent variables\n",
    "boston = pd.DataFrame(boston_dataset.data,\n",
    "                      columns=boston_dataset.feature_names)\n",
    "\n",
    "# add the target\n",
    "boston['MEDV'] = boston_dataset.target\n",
    "\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the information about the boston house prince dataset\n",
    "# in case you would like to get familiar with the variables before \n",
    "# continuing with the notebook\n",
    "\n",
    "# the aim is to predict the \"Median house value (price)\"\n",
    "# MEDV column of this dataset\n",
    "\n",
    "# and we have variables with characteristics about\n",
    "# the homes and the neighborhoods\n",
    "\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train and evaluate the model, let's first split into\n",
    "# train and test data, using 3 variables of choice:\n",
    "# LSTAT, RM and CRIM\n",
    "\n",
    "# let's separate into training and testing set\n",
    "# using the sklearn function below\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston[['RM', 'LSTAT', 'CRIM']],\n",
    "    boston['MEDV'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's scale the features\n",
    "# normal procedure for linear models\n",
    "# I will explain this later in in the course\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a linear model using the data as loaded from sklearn\n",
    "\n",
    "# instantiate a lineear model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "# make predictions on the train set and calculate\n",
    "# the mean squared error\n",
    "print('Train set')\n",
    "pred = linreg.predict(scaler.transform(X_train))\n",
    "print('Linear Regression mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "\n",
    "# make predictions on the test set and calculate\n",
    "# the mean squared error\n",
    "print('Test set')\n",
    "pred = linreg.predict(scaler.transform(X_test))\n",
    "print('Linear Regression mse: {}'.format(mean_squared_error(y_test, pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the residuals\n",
    "\n",
    "error = y_test - pred\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test residuals normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will make a histogram to determine if the residuals\n",
    "# are normally distributed with mean value at 0\n",
    "\n",
    "sns.histplot(error, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the residuals show a fairly normal distribution centered at 0. Not perfect, by visual inspection there is some skew towards the left, with a few higher unusual values towards the right of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can better visualize residuals distribution with \n",
    "# a Q-Q plot. If the residuals are normally distributed\n",
    "# the dots should adjust to the 45 degree line\n",
    "\n",
    "stats.probplot(error, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Q-Q plot we see more easily how the residuals deviate from the red line towards the ends of the distribution, thus, they are not normally distributed.\n",
    "\n",
    "## Homoscedasticity\n",
    "\n",
    "Homoscedasticity implies that at each level of the predictor variable(s), the variance of the residual terms should be constant. So we need to plot the residuals against the variables.\n",
    "\n",
    "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables X and the dependent variable Y is the same across all the independent variables.\n",
    "\n",
    "The way to identify if the variables are homoscedastic, is to make a linear model with all the independent variables involved, calculate the residuals, and plot the residuals vs each one of the independent variables. If the distribution of the residuals is homogeneous across the variable values, then the variables are homoscedastic.\n",
    "\n",
    "There are other tests for homoscedasticity:\n",
    "\n",
    "- Residuals plot\n",
    "- Levene’s test\n",
    "- Barlett’s test\n",
    "- Goldfeld-Quandt Test\n",
    "\n",
    "But those escape the scope of this course. So for this demo I will focus on residual plot analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals vs one of the independent\n",
    "# variables, LSTAT in this case\n",
    "\n",
    "plt.scatter(x=X_test['LSTAT'], y=error)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals seem fairly homogeneously distributed across the values of LSTAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the residuals vs RM\n",
    "plt.scatter(x=X_test['RM'], y=error)\n",
    "plt.xlabel('RM')\n",
    "plt.ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this variable, the residuals do not seem to be homogeneously distributed across the values of RM. In fact, low and high values of RM show higher error terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals vs one of the independent\n",
    "# variables, CRIM in this case\n",
    "\n",
    "plt.scatter(x=X_test['CRIM'], y=error)\n",
    "plt.xlabel('CRIM')\n",
    "plt.ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most values of CRIM are skewed towards the left, so it is hard to say if the residuals show the same variance for all values of CRIM, because we have very few data points for CRIM when its values are high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Residual analysis with Yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, I want to introduce the use of yellobrick\n",
    "# a library for visualisation of machine learning model \n",
    "# performance\n",
    "\n",
    "# if you don't have yellowbricks installed, comment out\n",
    "# this cell to avoid errors while running the notebook\n",
    "\n",
    "# yellowbricks allows you to visualise the residuals of the\n",
    "# models after fitting a linear regression\n",
    "\n",
    "\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "\n",
    "visualizer.fit(scaler.transform(X_train), y_train)  # Fit the training data to the model\n",
    "visualizer.score(scaler.transform(X_test), y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the right, we have the distribution of the residuals in the train and test sets. We see that it is not perfectly centered at 0.\n",
    "\n",
    "On the left, we have the residuals vs the predicted value, we also see that the variance is not constant. Towards the extremes of the predictions, the model is under-estimating the outcome (most residuals are negative). And towards the center of the predictions, the model is over-estimating the outcome. So the residuals variance is not constant for all values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data to improve model fit\n",
    "\n",
    "We will use the Box-Cox transformation, which I will discuss in more detail in a later section in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# apply the box-cox transformation to the variables\n",
    "boston['LSTAT'], _ = stats.boxcox(boston['LSTAT'])\n",
    "boston['CRIM'], _ = stats.boxcox(boston['CRIM'])\n",
    "boston['RM'], _ = stats.boxcox(boston['RM'])\n",
    "\n",
    "# let's separate into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston[['RM', 'LSTAT', 'CRIM']],\n",
    "    boston['MEDV'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's scale the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build a new model using the transformed variables\n",
    "\n",
    "# specify the model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "# make predictions and calculate the mean squared\n",
    "# error over the train set\n",
    "print('Train set')\n",
    "pred = linreg.predict(scaler.transform(X_train))\n",
    "print('Linear Regression mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "\n",
    "# make predictions and calculate the mean squared\n",
    "# error over the test set\n",
    "print('Test set')\n",
    "pred = linreg.predict(scaler.transform(X_test))\n",
    "print('Linear Regression mse: {}'.format(mean_squared_error(y_test, pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the residuals\n",
    "\n",
    "error = y_test - pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will make a histogram to determine if the residuals\n",
    "# are normally distributed with mean value at 0\n",
    "\n",
    "sns.histplot(error, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can better visualize residuasl distribution with \n",
    "# a Q-Q plot. If the residuals are normally distributed\n",
    "# the dots should adjust to the 45 degree line\n",
    "\n",
    "stats.probplot(error, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an improvement: the residuals are now \"more\" normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now do the analysis with yellowbrick\n",
    "\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(scaler.transform(X_train), y_train)\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "\n",
    "visualizer.fit(scaler.transform(X_train), y_train)  # Fit the training data to the model\n",
    "visualizer.score(scaler.transform(X_test), y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a noticeable improvement in the distribution of the residuals, now better centered at 0, and with a more even variance across all values of the prediction.\n",
    "\n",
    "**HOMEWORK**\n",
    "\n",
    "Plot the residuals vs the transformed variables to determine homoscedasticity.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "The model performance would improve even further if we transformed the target as well. I leave that to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing other model assumptions\n",
    "\n",
    "If we have time, and want to better understand the relationships between our variables among themselves and the outcome, we can go ahead and check the linear relationship and colinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will create a dataframe with the variable x that\n",
    "# follows a normal distribution and shows a\n",
    "# linear relationship with y\n",
    "\n",
    "# this will provide the expected plots\n",
    "# i.e., how the plots should look like if the\n",
    "# assumptions are met\n",
    "\n",
    "np.random.seed(29) # for reproducibility\n",
    "\n",
    "n = 200\n",
    "x = np.random.randn(n)\n",
    "y = x * 10 + np.random.randn(n) * 2\n",
    "\n",
    "toy_df = pd.DataFrame([x, y]).T\n",
    "toy_df.columns = ['x', 'y']\n",
    "toy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear relationship\n",
    "\n",
    "We evaluate linear assumption with scatter plots and residual plots. Scatter plots plot the change in the dependent variable y with the independent variable x.\n",
    "\n",
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the simulated data\n",
    "\n",
    "# this is how the plot looks like when\n",
    "# there is a linear relationship\n",
    "\n",
    "sns.lmplot(x=\"x\", y=\"y\", data=toy_df, order=1)\n",
    "# order 1 indicates that we want seaborn to\n",
    "# estimate a linear model (the line in the plot below)\n",
    "# between x and y\n",
    "\n",
    "plt.ylabel('Target')\n",
    "plt.xlabel('Independent variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we make a scatter plot for the boston\n",
    "# house price dataset\n",
    "\n",
    "# (remember that we transformed the variables already)\n",
    "\n",
    "# we plot LAST (% lower status of the population)\n",
    "# vs MEDV (median value of the house)\n",
    "\n",
    "sns.lmplot(x=\"LSTAT\", y=\"MEDV\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between LSTAT and MEDV is fairly linear apart from a few values around the minimal values of LSTAT, towards the top left side of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot RM (average number of rooms per dwelling)\n",
    "# vs MEDV (median value of the house)\n",
    "\n",
    "sns.lmplot(x=\"RM\", y=\"MEDV\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between the target and transformed RM is not very linear. We could consider, transforming this variable further, removing the variable from the model, or using a non-linear model to predict MEDV from RM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot CRIM (per capita crime rate by town)\n",
    "# vs MEDV (median value of the house)\n",
    "\n",
    "sns.lmplot(x=\"CRIM\", y=\"MEDV\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship is also not perfectly linear between CRIM and MEDV.\n",
    "\n",
    "**HOMEWORK**\n",
    "\n",
    "Go ahead and compare the original relationship of LSTAT, RM and CRIM with the target. Even though the relationships after the transformation are not perfectly linear, you will notice that it went a bit in that direction after the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### residual plots (errors)\n",
    "\n",
    "Another thing that we can do to determine whether there is a linear relationship between the variable and the target is to evaluate the distribution of the errors, or the residuals. The residuals refer to the difference between the predictions and the real value of the target. It is performed as follows:\n",
    "\n",
    "1) make a linear regression model using the desired variables (X)\n",
    "\n",
    "2) obtain the predictions \n",
    "\n",
    "3) determine the error (True house price - predicted house price)\n",
    "\n",
    "4) observe the distribution of the error.\n",
    "\n",
    "If the house price, in this case MEDV, is linearly explained by the variables we are evaluating, then the error should be random noise, and should typically follow a normal distribution centered at 0. We expect to see the error terms for each observation lying around 0.\n",
    "\n",
    "We will do this first, for the simulated data, to become familiar with how the plots should look like. Then we will do the same for LSTAT and then, we will transform LSTAT to see how transformation affects the residuals and the linear fit.\n",
    "\n",
    "**Note**\n",
    "\n",
    "This is a bit of an over-kill, if what we are trying to do is to predict an outcome from predictor variables. However, you may want to do this, to better understand the relationships between your variables and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATED DATA\n",
    "\n",
    "# step 1: make a linear model\n",
    "# call the linear model from sklearn\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "linreg.fit(toy_df['x'].to_frame(), toy_df['y'])\n",
    "\n",
    "# step 2: obtain the predictions\n",
    "# make the predictions\n",
    "pred = linreg.predict(toy_df['x'].to_frame())\n",
    "\n",
    "# step 3: calculate the residuals\n",
    "error = toy_df['y'] - pred\n",
    "\n",
    "# plot predicted vs real\n",
    "plt.scatter(x=pred, y=toy_df['y'])\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Real value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model makes good predictions. The predictions are quite aligned with the real value of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: observe the distribution of the errors\n",
    "\n",
    "# Residuals plot\n",
    "# if the relationship is linear, the noise should be\n",
    "# random, centered around zero, and follow a normal distribution\n",
    "\n",
    "# we plot the error terms vs the independent variable x\n",
    "# error values should be around 0 and homogeneously distributed\n",
    "\n",
    "plt.scatter(y=error, x=toy_df['x'])\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('Independent variable x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are distributed around 0, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: observe the distribution of the errors\n",
    "\n",
    "# plot a histogram of the residuals\n",
    "# they should follow a gaussian distribution\n",
    "# centered around 0\n",
    "\n",
    "sns.histplot(error, bins=30)\n",
    "plt.xlabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors adopt a Gaussian distribution and it is centered around 0. So it meets the assumptions, as expected.\n",
    "\n",
    "Let's do the same for LSTAT (remember that we already transformed this variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the linear model from sklearn\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "linreg.fit(boston['LSTAT'].to_frame(), boston['MEDV'])\n",
    "\n",
    "# make the predictions\n",
    "pred = linreg.predict(boston['LSTAT'].to_frame())\n",
    "\n",
    "# calculate the residuals\n",
    "error = boston['MEDV'] - pred\n",
    "\n",
    "# plot predicted vs real\n",
    "plt.scatter(x=pred, y=boston['MEDV'])\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a relatively good fit for most of the predictions, but the model does not predict very well towards the highest house prices. For high house prices, the model under-estimates the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals plot\n",
    "\n",
    "# if the relationship is linear, the noise should be\n",
    "# random, centered around zero, and follow a normal distribution\n",
    "\n",
    "plt.scatter(y=error, x=boston['LSTAT'])\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('LSTAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are not really centered around zero. And the errors are not homogeneously distributed across the values of LSTAT. Low and high values of LSTAT show higher errors. \n",
    "\n",
    "The relationship could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the residuals\n",
    "# they should follow a gaussian distribution\n",
    "sns.histplot(error, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are not centered around zero, and the distribution is not totally Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinearity\n",
    "\n",
    "To determine co-linearity, we evaluate the correlation of all the independent variables in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture features in a list\n",
    "features = boston_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we calculate the correlations using pandas corr\n",
    "# and we round the values to 2 decimals\n",
    "correlation_matrix = boston[features].corr().round(2)\n",
    "\n",
    "# plot the correlation matrix usng seaborn\n",
    "# annot = True to print the correlation values\n",
    "# inside the squares\n",
    "\n",
    "figure = plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(data=correlation_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the x and y axis of the heatmap we have the variables of the boston house dataframe. Within each square, the correlation value between those 2 variables is indicated. For example, for LSTAT vs CRIM at the bottom left of the heatmap, we see a correlation of 0.46. These 2 variables are not highly correlated.\n",
    "\n",
    "Instead, for the variables RAD and TAX (try and find them in the plot), the correlation is 0.91. These variables are highly correlated. The same is true for the variables NOX and DIS, which show a correlation value of -0.71.\n",
    "\n",
    "Let's see how they look in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between RAD (index of accessibility to radial highways)\n",
    "# and TAX (full-value property-tax rate per $10,000)\n",
    "\n",
    "sns.lmplot(x=\"RAD\", y=\"TAX\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now NOX (itric oxides concentration (parts per 10 million))\n",
    "# and DIS (weighted distances to five Boston employment centres)\n",
    "\n",
    "sns.lmplot(x=\"NOX\", y=\"DIS\", data=boston, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation, or co-linearity between NOX and DIS, is quite obvious in the above scatter plot. So these variables are violating the assumption of no multi co-linearity.\n",
    "\n",
    "What we would do is remove 1 of the 2 from the dataset before training the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
